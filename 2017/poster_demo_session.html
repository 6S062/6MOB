<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html>

<head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="6.S062 : Mobile and Sensor Computing Course">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/styles/default.min.css">
    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">
    <title>6.S062 - Semester Project Presentations</title>
    <style type="text/css">
        table.projects {
            margin-left:auto;
            margin-right:auto;
            width:80%;
            max-width:940px;
            border:none;
        }

        table.projects td.title {
            padding-right:50px;
            width:500px;
            border:none;
            padding-bottom:10px;
        }

        table.projects td.title img {
            width:50px;
            border:none;
            box-shadow:none;
            display:block;
            float:left;
            margin-left:-68px;
            margin-right:18px;
            padding-left:0;
            padding-right:0;
        }

        table.projects td.left {
            padding-left:10px;
            padding-right:36px;
            padding-top:0;
            margin-top:0;
            width:500px;
            border:none;
            text-align:justify;
            vertical-align:top;
        }

        table.projects td.left p {
            margin-top:0;
        }

        table.projects td.right {
            padding:0;
            margin:0;
            padding-top:5px;
            max-width:400px;
            border:none;
        }

        table.projects td.right img {
            width:100%;
            max-width:400px;
            min-width:200px;
            padding:0;
            margin:0;
            border:none;
            box-shadow:none;
            display:block;
        }

        table.projects td.buttons {
            border:none;
            padding:0;
            text-align:right;
            max-width:400px;
            vertical-align:middle;
        }

        table.projects td.buttons a {
            vertical-align:middle;
            text-decoration:none;
            cursor:pointer;
            display:inline-block;
            width:60px;
            height:50px;
            padding:0;
            margin:0;
            margin-left:6px;
            background-color:#bbbbc2;
            text-align:center;
            transition: all 0.25s ease-in;
            color:rgba(0,0,0,0);
            background-image:none;
            position:relative;
            line-height:20px;
            padding-top:10px;
        }

        table.projects td.buttons a:hover {
            color: #000;
        }

        table.projects td.buttons a img {
            opacity:1.0;
            transition: all 0.25s ease-in;
            box-shadow:none;
            border:none;
            padding:0;
            margin:0;
            position:absolute;
        }

        img.repo {
            width:51px;
            height:55px;
            left:6px;
            top:5px;
        }

        img.pdf {
            width:42px;
            height:40px;
            left:10px;
            top:10px;
        }

        img.video {
            width:42px;
            height:40px;
            left:10px;
            top:10px;
        }

        img.data {
            width:42px;
            height:41px;
            left:9px;
            top:10px;
        }

        table.projects td.buttons a:hover img {
            opacity:0.0;
        }
    </style>
</head>
<body>
    <div id="header_wrap" class="outer">
        <section id="main_content" class="inner">
            <header class="inner">
                <h1 id="project_title">Semester Project Presentations</h1>
                <h2 id="project_tagline">6.S062 Mobile and Sensor Computing</h2>
                <p style="color:#fff">Wednesday, May 17, 2017, 11am-12:30pm, 32-G882</p>
            </header>
        </section>
    </div>
    <div id="main_content_wrap" class="outer">
        <section id="main_content" class="inner">
            <p>
            On Wednesday, May 17, 2017, from 11am-12:30pm, the students of 6.S062 Spring 2017
            (Mobile and Sensor Computing) presented posters and demos of
            their semester projects in the 32-G882.
            </p>

            <hr>
        </section>

            <table class="projects">
                <tr>
                    <td class="title">
                        <!-- <img src="images/noun_251228_cc.svg"> -->
                        <h5>Building Health Applications from IoT Data Using Object Spreadsheet</h5>
                        <h6>Mengjiao Yang</h6>
                    </td>
                    <!-- <td class="buttons">
                        <a target="_blank" href="posters/bartel-efah.pdf">Poster<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://github.com/kwbartel/BenchBuddy">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td> -->
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        As many IoT devices have been developed in healthcare, the need for a generic platform to manage healthcare data collected from different devices increases. One might, for example, collect heart rate and blood pressure data from a Fitbit tracker, and use the GPS chip built in an iPhone to record his/her locations. In this project, we explore using object spreadsheet developed by the Software Design Group at MIT as a storage backend to consolidate data gathered from different devices. To demonstrate the feasibility of such consolidation, we built a position logger web application that logs a user’s trace with health annotations such as heart rate and blood pressure. In building  this application, we noticed that object spreadsheet is easily programmable given its formula-based feature and object-oriented nature.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_2313.JPG">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <!-- <img src="images/noun_521.svg"> -->
                        <h5>Title: ME$H: Proximity-based Bluetooth Messaging App</h5>
                        <h6>Tina Quach, J. Maunsell, and Amanda Ke</h6>
                    </td>
                    <!-- <td class="buttons">
                        <a target="_blank" href="posters/brettin-chang.pdf">Poster<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://github.com/BrendanSChang/verge">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td> -->
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        During disasters, core components of communication systems fail, including cellular and WiFi networks. A Bluetooth mesh network of mobile devices would enable people to contact each other during an outage. Furthermore, large events often have limited reception and bandwidth due to the number of users competing for these resources. We’ve implemented a proximity-based messaging iOS app that allows people to communicate over Bluetooth low energy (BLE) using an identity of their choice. The app enables users to create and join scalable mesh networks based on proximity of other users and allows them to message anyone else in their mesh network using a multi-hop, message-forwarding protocol. This solution thrives in large crowds, since the network becomes more efficient and wide reaching as it scales up. As a proof of concept, our project is iOS only and tested in a specific building, Senior House dormitory. Using our BLE mesh network and our app, we test the efficacy of our protocol for multi-hop in terms of loss rate, latency, and power consumption.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_2309.JPG">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <!-- <img src="images/noun_110451_cc.svg"> -->
                        <h5>WiFinder - Indoor Localization via Commodity Hardware</h5>
                        <h6>Douglas Chen, Bishesh Khadka, Varun Mahon</h6>
                    </td>
                    <!-- <td class="buttons">
                        <a target="_blank" href="posters/callahan-esquivel-frankel-wang.pdf">Poster<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://github.mit.edu/abe707/6mob-final">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                        <a target="_blank" href="https://github.mit.edu/abe707/6mob-final-data-collection">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                        <a target="_blank" href="https://github.mit.edu/abe707/6mob-final-server">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td> -->
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        Existing solutions for indoor localization typically require specialized hardware such as ultrasound and RF transmitters/receivers, or rely on low accuracy metrics such as WiFi signal strength. We propose the WiFinder system for localization within rooms using WiFi and acoustic measurements on laptops. WiFinder measures distance by comparing the time difference in arrival of acoustic and WiFi signals. WiFinder tolerates timing idiosyncrasies from running on a non-real time operating system, and is resilient to ambient noise. We find that WiFinder is able to achieve accuracy within a foot, and with a maximum range exceeding 25 feet. 
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_7576.JPG">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <!-- <img src="images/noun_75728_cc.svg"> -->
                        <h5>Determining room occupancy via combined sensors</h5>
                        <h6>Famien Koko, Justin Tunis and Jon Beaulieu</h6>
                    </td>
                    <!-- <td class="buttons">
                        <a target="_blank" href="posters/consul-pramanick.pdf">Slides<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://github.com/smritip/mit-touring-machine">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td> -->
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        In this project we attempt to solve the problem of determining room occupancy by using sensors. We test a variety of sensors and approaches to see what are the most viable ways to accomplish our goal. One effective approach we found was using an array of IR sensors placed out of a doorway. You can determine when people walk in or out by looking at which sensors were activated first. Starting from occupancy 0, you can increase and decrease the occupancy based on people walking in or out. Edge cases can be handled by combining other sensors, (such as noise sensors and light ) to corroborate data from the IR sensors.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_7589.JPG">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <!-- <img src="images/noun_212277_cc.svg"> -->
                        <h5>Hybrid Prediction Model for Shuttle Arrival Time</h5>
                        <h6>Byungkyu Park, Hyunjoon Song</h6>
                    </td>
                    <!-- <td class="buttons">
                        <a target="_blank" href="posters/dumesnil-lucier-nordin.pdf">Slides<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://youtu.be/1jEQFhUjNEI">Video<br>Demo<img class="video" src="images/video.svg"></a>
                        <a target="_blank" href="https://github.com/jlucier/6.S062-car-swarm">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                        <a target="_blank" href="https://github.com/jlucier/car_tracker">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td> -->
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        Many MIT students including ourselves are often frustrated with the inaccuracy and unreliability of the current shuttle tracking application. The application utilizes the NextBus API, which provides predictions for schedule information and shuttle location using real-time GPS data. Unfortunately, the API occasionally becomes unavailable and provides inaccurate predictions especially during rush hours, making students wait for almost an hour without knowing when the shuttles would arrive. To address these challenges, we systematically analyzed our collected data over the past few weeks and created a statistical model based on the historical average of shuttle dwelling time at each stop. In addition, we query the Google Directions API to get a prediction of running time at each link to take in account of real-time traffic information . By combining these two sources, we have created a hybrid prediction model that accurately predicts shuttle arrival time.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_2314.JPG">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <!-- <img src="images/noun_15818_cc.svg"> -->
                        <h5>Noklipping: Noise Bleed Detection in Kresge Auditorium</h5>
                        <h6>Amber Meighan, Nichole Clarke and Olivier Midy</h6>
                    </td>
                   <!--  <td class="buttons">
                        <a target="_blank" href="posters/freel-titus.pdf">Poster<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://youtu.be/MFM_eqwavqg">Video<br>Demo<img class="video" src="images/video.svg"></a>
                        <a target="_blank" href="https://github.com/atitus5/6s062-Project">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                        <a target="_blank" href="https://github.com/atitus5/6s062-Project/tree/master/logs">Data<br>Repo<img class="data" src="images/data.svg"></a>
                    </td> -->
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        In this work, algorithms are developed and evaluated to detect physical activities from data acquired using five small biaxial accelerometers worn simultaneously on different parts of the body. Acceleration data was collected from 20 subjects without researcher supervision or observation. Subjects were asked to perform a sequence of everyday tasks but not told specifically where or how to do them. Mean, energy, frequency-domain entropy, and correlation of acceleration data was calculated and several classifiers using these features were tested. Decision tree classifiers showed the best performance recognizing everyday activities with an overall accuracy rate of 84%. The results show that although some activities are recognized well with subject-independent training data, others appear to require subject-specific training data. The results suggest that multiple accelerometers aid in recognition because conjunctions in acceleration feature values can effectively discriminate many activities. With just two biaxial accelerometers – thigh and wrist – the recognition performance dropped only slightly. This is the first work to investigate performance of recognition algorithms with multiple, wire-free accelerometers on 20 activities using datasets annotated by the subjects themselves.

                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_2311.JPG">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <!-- <img src="images/noun_110488_cc.svg"> -->
                        <h5>ShakeWake: A Gesture and Location-Based Alarm Clock</h5>
                        <h6>Matt Basile, Julia Guo and Daniel Lerner</h6>
                    </td>
                    <!-- <td class="buttons">
                        <a target="_blank" href="posters/song-xiang.pdf">Slides<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://www.youtube.com/watch?v=jmgfW-EGMBM">Video<br>Demo<img class="video" src="images/video.svg"></a>
                        <a target="_blank" href="https://github.com/xiangcy/AirGestureClassifier">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td> -->
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        When we wake up in the morning, it is difficult to motivate ourselves to get out of bed. To solve this problem, our team has developed ShakeWake, a mobile application that makes waking up easier (or harder). Users are able to set a custom alarm that can be turned off by shaking the phone vigorously or by walking to a specific location. The movement of the phone is measured through the internal accelerometer, and location proximity is detected through external Bluetooth sensors.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_2319.JPG">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <!-- <img src="images/noun_28242_cc.svg"> -->
                        <h5>A Location-Based Gesture Recognition App with Apple Watch Integration</h5>
                        <h6>Samuel Song, Jing Lin</h6>
                    </td>
                    <!-- <td class="buttons">
                        <a target="_blank" href="posters/gupta-martinez.pdf">Slides<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://github.com/ankushg/airmuler">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td> -->
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        Making hand gestures is a common way that people interact with each other. With Airo, we use accelerometer data from the Apple Watch to recognize gestures and interact with the phone.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_7585.JPG">
                    </td>
                </tr>

                
                
            </table>

        <section id="main_content" class="inner">
            <p style="color:#808080; font-size:80%; line-height: 1.2em;">
            Images from the Noun project:<br>
            Barbell by Dara Ullrich<br>
            Shake Phone by Web Icon Set<br>
            Massachusetts Institute of Technology by NATAPON CHANTABUTR<br>
            Self-driving car by Jakob Vogel<br>
            entrance by Marc Andre Roy<br>
            Bump to Right by Web Icon Set<br>
            Donkey by Luis Prado<br>
            Tracking Location by Friedrich Santana<br>
            internet of things by Felix Westphal
            PDF File by Laurent Canivet
            Video by Milky - Digital innovation
            Graph by Simple Icons
            </p>

        </section>
    </div>
</body>
</html>
