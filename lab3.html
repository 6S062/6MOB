<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html>

<head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="6.S062 : Mobile and Sensor Computing Course">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/styles/default.min.css">
    <script src="https:////cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/highlight.min.js"></script>
    <script>
    hljs.initHighlightingOnLoad();
    </script>
    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">
    <title>6.S062 - Lab 3</title>
<style type="text/css">
<!--
.nowrap {
    white-space:nowrap;
}
div.aside {
    margin-top:1em;
    padding-left:16pt;
    padding-right:16pt;
    padding-top:.4em;
    padding-bottom:.4em;
    margin-bottom:1em;
    font-size:11pt;
    background-color:#F0F0F0;
}
div.aside h6 {
    margin-bottom:-.5em;
}
div.aside code {
    background-color:#FCFCFC;
    box-shadow:none;
}
td code {
    box-shadow:none;
    background-color:none;
}
sup {
    vertical-align: super;
    font-size: smaller;
}
sub {
    vertical-align: sub;
    font-size: smaller;
}
pre code {
    width: auto;
}
a code {
    color: inherit;
}
span.fraction sup {
    font-size: 75%;
}
span.fraction sup::after {
    content: "/";
    vertical-align: sub;
    position: relative;
    top:2px;
    left:-1px;
    font-size: larger;
}
span.fraction sub {
    vertical-align: sub;
    font-size: 75%;
    margin-left:-2px;
}

span.inequality {
    position:relative;
    display:block;
    width:8ex;
}
span.inequality::after {
    content: "a";
    visibility: hidden;
}
span.inequality span.i1 {
    position:absolute;
    left:-.5ex;
    width:2.5ex;
    text-align:center;
}
span.inequality span.i2 {
    position:absolute;
    left:2ex;
}
span.inequality span.i3 {
    position:absolute;
    left:3.5ex;
}
span.inequality span.i4 {
    position:absolute;
    left:5ex;
}
span.inequality span.i5 {
    position:absolute;
    left:6.5ex;
    width:2.5ex;
    text-align:center;
}

//-->
</style>
</head>
<body>
    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <section id="main_content" class="inner">
            <header class="inner">
                <h1 id="project_title">6.S062</h1>
                <h2 id="project_tagline">Lab 3: Gesture Recognition and Inertial Sensing</h2>
                <font color="white">Assigned: 3/28/2016
                    <br>
                    <b>Due</b>: 4/11/2016</font>
                <br>
            </header>
        </section>
    </div>
    <div id="main_content_wrap" class="outer">
        <section id="main_content" class="inner">
        <ul>
            <li><a href="#overview">Overview</a>
            <li><a href="#sec1">Section 1 &mdash; Handwriting</a>
            <ul>
                <li><a href="#task1a">Task 1A &mdash; Rescaling</a>
                <li><a href="#task1b">Task 1B &mdash; Feature Extraction</a>
                <li><a href="#task1c">Task 1C &mdash; Data Collection</a>
                <li><a href="#task1d">Task 1D &mdash; Training</a>
                <li><a href="#task1e">Task 1E &mdash; Matching</a>
                <li><a href="#task1f">Task 1F &mdash; Optional Improvements</a>
            </ul>
            <li><a href="#sec2">Section 2 &mdash; Inertial Measurement</a>
            <ul>
                <li><a href="#motionBackground">Background</a>
                <li><a href="#rotations">Rotations and Orientations</a>
                <li><a href="#backtoyou">Responsibilities of <code>accumulateMotion:</code></a>
                <li><a href="#task2a">Task 2A &mdash; Coordinate Transformation</a>
                <li><a href="#task2b">Task 2B &mdash; Integration</a>
                <li><a href="#task2c">Task 2C &mdash; Stabilization</a>
                <li><a href="#task2d">Task 2D &mdash; Optional Improvements</a>
            </ul>
            <li><a href="#sec3">Section 3 &mdash; Mapping 3-D Gestures to 2-D</a>
            <ul>
                <li><a href="#task3a">Task 3A &mdash; Estimate Axes</a>
                <li><a href="#task3b">Task 3B &mdash; Project Coordinates</a>
                <li><a href="#task3c">Task 3C &mdash; Optional Improvements</a>
            </ul>
            <li><a href="#appendix">Appendix: Functions</a>
        </ul>

        <h2 id="overview">Overview</h2>

        <p>In this lab, you will</p>
        <ul>
        <li>Build a working gesture/handwriting recognition system using machine learning
        <li>Work with data from the accelerometer, gyroscope, and compass
        <li>Become Kings of Kernels and Queens of Quaternions
        </ul>

        <p>Teams of two are encouraged but not required.  The lab is due Wednesday,
        April 6.  Check-offs are by appointment with Peter Iannucci, or during his
        office hours Tues 4-5 / Thurs 11-12.  Come to us early if you get stuck!</p>

        <p>As with most machine learning systems, the classifiers used in this lab are
        not perfect, and may exhibit frustrating behaviors.  Persistence and creative
        problem solving (a.k.a. hacking up something that generally works) are
        encouraged.  If you would like to brainstorm about any problems you encounter,
        we'd be happy to talk.</p>

        <p>Please go ahead and download the <a href="lab3.zip">Xcode project</a> for
        this lab.  The lab will consist of three sections.  First, you will implement a
        shape recognition algorithm using two-dimensional handwriting on the touch
        screen as the data source.  Next, you will develop 3-D motion tracking code
        using the iPhone's sensors, and gain familiarity with the capabilities and
        limitations of these sensors by playing with the resulting app.  Finally, you
        will feed data from your three-dimensional motion tracker into the gesture
        recognition system of the first section.  At each point, you will be welcome to
        expand the capabilities of the basic recognition framework we have put in your
        hands.</p>

        <h2 id="sec1">Section 1 &mdash; Handwriting</h2>

        <p>For this section, you will work through the details of implementing a
        handwriting recognition system for letters drawn with a finger on an iPhone
        screen.  Here's how this will work.</p>

        <p>You will be implementing <code>GestureProcessor</code>, a class encapsulating
        the logic for interpreting sequences of points.  In particular, you will fill in
        all of the interesting parts of this method in GestureProcessor.m, which you can
        find under InertialMotion/Gesture Recognition in Xcode's project navigator:</p>

        <pre id="processGesture2D"><code class="hljs objectivec">- (void)processGesture2DWithSamples:(const Sample2D *)samples
                              count:(NSUInteger)count
                            minSize:(double)minSize {
    // -- TASK 1A --
    double size, clippedSize;
    Sample2D rescaledSamples[count];
    double minX = +INFINITY, maxX = -INFINITY, minY = +INFINITY, maxY = -INFINITY;
    // Compute size, clippedSize
    // Rescale points to lie in [0,1] x [0,1]
    
    // -- <span id="task1b_code">TASK 1B</span> --
    double features[N_FEATURES] = {};
    // Classify each point according to which zone of a 3x3 Tic-Tac-Toe board it would fall in
    // Compute the time spent in each zone and the distance traveled horizontally and vertically
    
    // -- TASK 1C --
#if TRAINING
    // Use this code if you want to do additional training
    // Log feature vector (with empty string for label) for training
    // Make sure to fill in the empty label when you copy the output into training.py

    NSMutableString *s = [@"('', [" mutableCopy];
    for (int i=0; i&lt;N_FEATURES; i++)
        [s appendFormat:@"%+.5f, ", features[i]];
    [s replaceCharactersInRange:NSMakeRange([s length]-2, 2) withString:@""];
    [s appendString:@", 1.0]),\n"];
    
    AppDelegate *delegate = [[UIApplication sharedApplication] delegate];
    [delegate appendTrainingLog:s];
#endif
    // -- TASK 1D --
    // The output of the training procedure goes at the top of GestureProcessor.m.
    
    // -- <span id="task1e_code">TASK 1E</span> --
    int best_label = N_LABELS;
    double best_score = -INFINITY;
    // Dot product with gesture templates in weights[][]
    
#if !TRAINING
    // Report strongest match
    NSLog(@"Matched '%@' (score %+.5f)", labels[best_label], best_score);
#endif
    [self.delegate gestureProcessor:self didRecognizeGesture:labels[best_label]];
}</code></pre>

        <p>This method will be called for you by the view controller when the user has
        traced a stroke on the screen and then lifted their finger.  Each sample will
        consist of x and y coordinates for a touch event reported by iOS, along with
        the absolute time in seconds when it occurred.  You don't need to know how the
        UI works for this lab, but you may find it useful later to familiarize yourself
        with how the <code>Gesture2DViewController</code> captures multi-touch events
        via the <a
        href="https://developer.apple.com/library/ios/documentation/UIKit/Reference/UIResponder_Class/"
        target="_blank"><code>UIResponder</code></a>
        <a href="https://developer.apple.com/library/ios/documentation/EventHandling/Conceptual/EventHandlingiPhoneOS/event_delivery_responder_chain/event_delivery_responder_chain.html" target="_blank">mechanism</a> and the
        <code class="nowrap">-(void)touches...:withEvent:</code> methods.</p>

        <div class="aside">
        <h6>Aside:</h6>
        <p>The touch interface on iOS updates at (typically) 60 times per second, except
        on very recent iPads, which are faster.  If a user is scribing letters fluidly
        and quickly, this is enough to resolve the shape of the letters, but not to
        resolve the nuances of a quick stroke of the pen.  If we connect-the-dots
        together on the screen with straight lines, we will not get a nice curve, but
        instead a jagged mess.  For this app, we've implemented a very simple smoothing
        technique using quadratic Bezier curves (look 'em up, they're fantastic) to make
        our results more aesthetically pleasing.  Keep an eye out, the next time you
        write your signature on a touch-screen, whether the system samples fast enough
        to resolve quick strokes, or whether you see a series of straight line segments
        (called a polyline).</p>
        </div>

        <p>Your ultimate goal in <code>processGesture2DWithSamples:count:minSize:</code>
        will be to call the gesture processor's delegate's
        <code>gestureProcessor:didRecognizeGesture:</code> method, i.e.
        <code>[self.delegate gestureProcessor:self didRecognizeGesture:gesture]</code>.
        The second argument is a string identifying the recognized gesture &mdash; for
        instance, a letter of the alphabet.</p>

        <div class="aside">
        <h6>Aside:</h6>
        <p>This pattern, of passing <code>self</code> to a delegate method declared in a
        protocol, is common in iOS, and is used because one instance of class B might
        serve as the delegate to multiple instances of class A, and needs to be able to
        tell which instance of A each delegate callback has come from.</p>
        </div>

        <h3 id="task1a">Task 1A &mdash; Rescaling</h3>
        <p>Your first task in 
        <a href="#processGesture2D"><code>processGesture2DWithSamples:count:minSize:</code></a>
        will be to rescale the stroke.  The user may have drawn a character small or
        large, and you need to find the minimum and maximum values of the x and y
        coordinates separately in order to measure the size of the stroke.</p>

        <center><img src="images/declaration.jpg" style="height:400px"></center>

        <p>The number you're trying to compute is the larger of the width or height of the
        stroke, i.e.</p>

        <pre><code class="hljs objectivec">size = MAX(maxX - minX, maxY - minY)</code></pre>

        <p>where you'll have to compute minX, maxX, minY, maxY by looping over the array
        of samples<a href="#appendix">*</a>.  You'll then need to allocate a new array
        to hold your transformed samples (since the input array is read-only), and copy
        the sample data into the new array, mapping the coordinates via the
        transformation <code>x → (x-minX)/size</code> (and similarly for y).</p>

        <p>This procedure will work great for capital letters, but it not for
        punctuation.  For instance, in the case of a dot, the size of the stroke is
        deliberately small.  We shouldn't magnify that dot into a giant circle by
        dividing its coordinates through by its tiny size.  If we did, then we might
        mistake the dot for a capital letter O, which is also circular.  In this lab, we
        will only be recognizing upper-case letters, not punctuation.  However, it's
        good to plan ahead.  Therefore, we've added an additional argument to the
        method, <code>minSize</code>, to indicate the range of sizes the caller expects
        a realistic stroke to exhibit.</p>

        <div class="aside">
        <h6>Aside:</h6>
        <p>Where does this <code>minSize</code> number come from?  For now, the x and y
        coordinates are in "points", which on an iPhone means roughly 150 micrometers,
        or 1/320 of the width of the screen.  Later on, the x and y coordinates will be
        in meters.  For this reason, the code should expect strokes of vastly differing
        sizes when coordinates are passed in with either set of units, and so we need to
        provide an additional piece of information &mdash; roughly speaking, how small
        of a movement the user can comfortably make in these units.  The only part of
        the code where this information is available is at the source of the samples, in
        <code>processGesture2D...</code>'s caller.</p>

        <p>The caller, for now, is <code>Gesture2DViewController</code>, which is
        taking touch input from the user.  Because a human finger is much larger than a
        pixel, <code>Gesture2DViewController</code> sets <code>minSize</code> to reflect the
        smallest size letter a human would be likely to draw with a fingertip.  We've
        set this to 100 pixels &mdash; about 1.5 cm &mdash; but you can change this if
        you like.  Later on, the caller will be <code>Gesture3DViewController</code>,
        and <code>minSize</code> will be set to 1 cm.</p>
        </div>

        <p>If your estimate of the size of the stroke, based on the equation above, is
        too small, you should "clip" it to minSize, e.g. via</p>

        <pre><code class="hljs objectivec">clippedSize = MAX(size, minSize)</code></pre>

        <p>Then you should use <code>x → (x-minX)/clippedSize</code>, and similarly for
        y, as your coordinate transformation from the input to the rescaled sample
        array.</p>

        <p class="deliverable">Test your work: print out size and clippedSize after
        scribbling on the screen, and check that clippedSize stays in the proper range.
        Your rescaled x and y coordinates should lie in [0,1] x [0,1].</p>

        <h3 id="task1b">Task 1B &mdash; Feature Extraction</h3>
        Your next task is to reduce this large and variable-length pile of numbers (the
        x,y,t coordinates of the rescaled samples) into a small and fixed-length
        representation that somehow captures the important aspects of the input.  Our
        choice of representation for this lab is not the only (or the best) way of doing
        this &mdash; it's an <em>ad hoc</em> technique based on the following justified,
        educated guesswork:

        <p>Observation 1: left and right, vs. center, are important distinctions in
        recognizing the bits of a handwritten letter.</p>

        <p>Observation 2: top and bottom, vs. middle, are important distinctions in
        recognizing the bits of a handwritten letter.</p>

        <p>Observation 3: gross horizontal and vertical movements are more important than
        the exact path followed.</p>

        <p>Observation 4: time spent drawing any given part of a letter might be
        interesting, let's throw that in, too.</p>

        <p>So our representation is based on dividing the gesture into thirds
        horizontally and vertically, as follows:</p>

        <pre>For each vertical 1/3 (top, middle, bottom):
    For each horizontal 1/3 (left, center, right):
        One number for the time spent in this zone divided by the time it took to draw the entire stroke
        One number for the net rightward movement observed in this zone
        One number for the net downward movement observed in this zone</pre>

        <p>The way you will compute these 27 numbers is by looping over segments (pairs
        of samples) in our input.  For instance, you could loop from i=1 to n-1, and
        then in the body of the loop, consider the line segment connecting samples[i-1]
        to samples[i].  When you consider a line segment, you should decide which of the
        nine zones it falls into (you could consider the midpoint, or one endpoint; the
        difference is so small that it won't matter much).  For instance, if
        <span class="nowrap">0 &le; x &lt; 1/3</span> and
        <span class="nowrap">1/3 &le; y &lt; 2/3</span> for samples[i], then
        the segment would be assigned to zone #3, the left-middle:</p>

        <center>
            <table>
                <tr><th style="text-align:center; background-color:#808080">Zones</th>
                    <td style="background-color:#D0D0D0;"><span class="inequality"><span class="i1">0</span><span class="i2">&le;</span><span class="i3">x</span><span class="i4">&lt;</span><span class="i5"><span class="fraction"><sup>1</sup><sub>3</sub></span></span></span></td>
                    <td style="background-color:#D0D0D0;"><span class="inequality"><span class="i1"><span class="fraction"><sup>1</sup><sub>3</sub></span></span><span class="i2">&le;</span><span class="i3">x</span><span class="i4">&lt;</span><span class="i5"><span class="fraction"><sup>2</sup><sub>3</sub></span></span></span></td>
                    <td style="background-color:#D0D0D0;"><span class="inequality"><span class="i1"><span class="fraction"><sup>2</sup><sub>3</sub></span></span><span class="i2">&le;</span><span class="i3">x</span><span class="i4">&le;</span><span class="i5">1</span></span></td>
                </tr>
                <tr><td style="background-color:#D0D0D0;"><span class="inequality"><span class="i1">0</span><span class="i2">&le;</span><span class="i3">y</span><span class="i4">&lt;</span><span class="i5"><span class="fraction"><sup>1</sup><sub>3</sub></span></span></span></td><td>0</td><td>1</td><td>2</td></tr>
                <tr><td style="background-color:#D0D0D0;"><span class="inequality"><span class="i1"><span class="fraction"><sup>1</sup><sub>3</sub></span></span><span class="i2">&le;</span><span class="i3">y</span><span class="i4">&lt;</span><span class="i5"><span class="fraction"><sup>2</sup><sub>3</sub></span></span></span></td><td>3</td><td>4</td><td>5</td></tr>
                <tr><td style="background-color:#D0D0D0;"><span class="inequality"><span class="i1"><span class="fraction"><sup>2</sup><sub>3</sub></span></span><span class="i2">&le;</span><span class="i3">y</span><span class="i4">&le;</span><span class="i5">1</span></span></td><td>6</td><td>7</td><td>8</td></tr>
            </table>
        </center>

        <p>Having decided which zone the segment belongs to, you should then compute the
        time difference between samples[i] and samples[i-1], the x difference, and the y
        difference, and add these three values to the appropriate locations in the array
        features[] (still in <a href="#task1b_code"><code>processGesture2D:...</code></a>).  When you are finished summing, these statements should be true
        (don't forget to initialize the features array to zero):</p>

        <pre>
features[0] = sum (over segments in the top left zone) of fraction of total time spent in this zone
features[1] = sum (over segments in the top left zone) of net movement in the +x direction while in this zone
features[2] = sum (over segments in the top left zone) of net movement in the +y direction while in this zone
features[3] = sum (over segments in the top center zone) of fraction of total time spent in this zone
features[4] = sum (over segments in the top center zone) of net movement in the +x direction while in this zone
features[5] = sum (over segments in the top center zone) of net movement in the +y direction while in this zone
...
features[15] = sum (over segments in the middle right zone) of fraction of total time spent in this zone
features[16] = sum (over segments in the middle right zone) of net movement in the +x direction while in this zone
features[17] = sum (over segments in the middle right zone) of net movement in the +y direction while in this zone
...
features[24] = sum (over segments in the bottom right zone) of fraction of total time spent in this zone
features[25] = sum (over segments in the bottom right zone) of net movement in the +x direction while in this zone
features[26] = sum (over segments in the bottom right zone) of net movement in the +y direction while in this zone</pre>

        <p>For later convenience, <code>features[27]</code> should be set to 1.0.</p>

        <h3 id="task1c">Task 1C &mdash; Data Collection</h3>
        <p>If you were doing this on your own, at this point you would want to print out
        your feature vector in a log message, and then collect some labeled example
        data.  For instance, you could sit and write letters in your own handwriting on
        your iPhone screen, keeping track of which letters each feature vector was
        extracted from.  Fortunately, you aren't doing this on your own, and we have
        already collected over 450 example feature vectors for your use (see
        training.py).  Unfortunately, these feature vectors are characteristic of <a
        href="#ourhand">our
        handwriting</a>, not yours.  If we are lucky, a system trained to recognize our
        handwriting will recognize yours, as well.  You are most welcome to collect your
        own data.</p>

        <p>You don't need to do anything for this task because we're nice like that.</p>

        <h3 id="task1d">Task 1D &mdash; Training</h3>
        <p>We implemented a multi-class perceptron in training.py to find a classifier that
        can accurately map feature vectors to class labels (letters of the alphabet).
        Once we have the perceptron weight vector from Python, we can plug it back into
        our Objective-C code.</p>

        <p>Here's what the multi-class perceptron looks like (some details removed for clarity):</p>

        <pre><code class="hljs python">def train(w, inputs, labels):
    while True:
        mislabeled = 0
        for i in range(inputs.shape[0]):
            label_estimated = (inputs[i] * w).sum(axis=1).argmax()
            label_ground_truth = labels[i]
            if label_estimated != label_ground_truth:
                w[label_ground_truth] += inputs[i]
                w[label_estimated] -= inputs[i]
                mislabeled += 1
        if mislabeled == 0:
            return w</code></pre>

        <p>What is going on here?  The code loops until it finds a rule, parameterized by
        a weight vector <code>w</code>, which correctly classifies each input to the
        corresponding label.  The labels in this simplified code are numbers, but they
        can easily be made to be characters or strings.  The rules we are considering
        consist of oriented planes in the space of feature vectors, such that the
        further a feature vector falls on the positive side of the plane for a
        particular letter (say, Q), the more strongly we believe that the stroke may
        represent that letter.  We compute the distance from the i<sup>th</sup> input
        vector to all of the planes in one swoop by evaluating the array expression
        <code>(inputs[i] * w).sum(axis=1)</code>, and then we pick the index (label) for
        which the score is greatest using <code>argmax()</code>.  Each time we encounter
        a mis-classification, that is, an estimated label which differs from the true
        label, we give the weights a kick in the direction which increases the score for
        the correct label and decreases the score for the (incorrect) label which had
        the highest score.  We hope that eventually the weights will change in such a
        way that all of the inputs are correctly labeled; in fact, under certain
        conditions, this outcome is guaranteed.  You are most welcome to do some
        background reading on the perceptron algorithm.</p>

        <p>For our purposes, this algorithm is nice because it is guaranteed to find a
        linear rule to separate the classes, if such a simple rule exists.  It is less
        nice that it makes no guarantee of which of the many dissimilar rules it will
        find.  We can partly accommodate this limitation by running the algorithm with
        the weight vector <code>w</code> initialized randomly in 1000 different ways,
        then averaging the results.</p>

        <div class="aside">
            <h6>Aside:</h6>
            <p>Note that when we added a 28<sup>th</sup> feature equal to 1.0, what we
            were really doing was moving the perceptron calculation into homogeneous
            coordinates: a trick for simplifying the code.  This allows us to rewrite
            the equation for a plane, w∙x+b=0, as w∙x=0 (where x is a feature vector and
            w, b are parameters of the plane).
            
            This works because when the column vector x is augmented with an extra row
            containing [1.0], and the column vector w is augmented with an extra row
            containing [b], then the b term in the plane equation is absorbed into
            w∙x.</p>
        </div>

        <p>When the Python program finishes (it will take a few minutes), it
        will print out some C code that can be used to replace the definitions
        at the top of GestureProcessor.m.</p>

        <p>You don't need to do anything for this task, because we've provided a set of
        trained weights already in GestureProcessor.m.  However, if you'd like to
        retrain the classifier, for instance to recognize your own handwriting better or
        to add custom gestures not in the Latin alphabet, you can do so using the code
        enabled by the macro <code>#define TRAINING 1</code> in AppDelegate.h.  When you
        run the app on a phone with this configuration, you can collect feature vectors
        from the debug log and then manually add these to training.py.  Then you can run
        training.py and copy the resulting weights back into GestureProcessor.m.</p>

        <h3 id="task1e">Task 1E &mdash; Have you noticed that these tasks are letters of the alphabet?</h3>
        <p>Next, you need to compare the feature vectors computed in
        <code>processGesture2D...</code> with the weights array, using the
        vector dot product (i.e. sum of products of components).  In the <a href="#task1e_code">appropriate place in <code>processGesture2D:...</code></a>, you should
        write some C code equivalent to the following Python code:</p>

        <pre><code class="hljs python">for i in range(N_LABELS):
    score = 0
    for j in range(N_FEATURES):
        score += features[j] * weights[i][j]
    # do something with score</code></pre>

        <p>Ultimately, you need to determine which label <code>i</code> corresponds to
        the highest score.  Then you can look up that label with <code>labels[i]</code>
        to get the character to which it corresponds, and pass this to
        <code>gestureProcessor:didRecognizeGesture:</code> on the delegate object as we
        discussed.  The delegate, <code>Gesture2DViewController</code>, will take care
        of printing the character at the bottom of the screen.</p>

        <p>For this task, you should demonstrate that you can draw letters on the phone
        and have them recognized and printed at the bottom of the screen.</p>

        <center><img src="images/screen2d.png" style="height:400px;"></center>

        <p id="ourhand">Here are some examples of our handwriting.  If yours is very different, you
        may not have much success getting it to be recognized unless you retrain the
        perceptron algorithm with your own data.  Don't forget, our naïve app expects
        that one stroke = one letter, so don't lift your fingertip until the letter is
        finished.</p>
        <center><img src="images/alphabet.png" style="height:400px;"></center>

        <h3 id="task1f">Task 1F &mdash; Optional Improvements</h3>
        <p>In Section 1, you've worked with a very particular machine learning solution.
        It involved computing an <em>ad hoc</em> set of features, feeding them to a
        particular classifier algorithm, and combining the output of that algorithm with
        live data in just the right way so as to get an immediate result.</p>

        <p>Note that this is one of very many possibilities.  If you'd like to explore
        alternatives in any part of this process, extra credit is available.  For
        instance, you might try scaling x and y differently to bring each character into
        a more uniform aspect ratio &mdash; being careful that your new code does not make
        recognition stop working for tall, thin characters like "I".  Or you could find
        out what happens when the stroke is sliced up into 4×4 squares or more, rather
        than 3×3.  You could also make changes to the app to support gestures consisting
        of more than one stroke, for instance by keeping additional state in instance
        variables of the GestureProcessor object, and not calling
        gestureProcessor:didRecognizeGesture: right away.  You might consider learning
        how to work with delays and asynchronous processing on iOS in order to trigger
        recognition when no new strokes have been added for a few hundred milliseconds.</p>

        <h2 id="sec2">Section 2 &mdash; Inertial Measurement</h2>
        <p>For this section, you will convert acceleration reported by the phone into an
        appropriate coordinate system where it can be integrated over time.  You will
        observe some pitfalls of this procedure, and consider how to mitigate them.</p>

        <p>iOS will report "device motion" (i.e. pre-fused sensor readings from the
        accelerometer, gyroscope, and compass) at up to 100 Hz.  As you know,
        acceleration is the rate of change of velocity, and velocity is the rate of
        change of position, so in principle we can recover relative position by
        integrating the acceleration twice.  This data source is not really suitable for
        double-integrating to get position, but we're going to try it anyway and see
        what we get.</p>

        <p>We're going to rely on CoreMotion to track rotation of the phone over time,
        while using gravity and the Earth's magnetic field to keep the gyro from
        drifting (otherwise, it would totally lose its orientation over the course of a
        few minutes).  CoreMotion will also do its best to work out which portion of the
        accelerometer signal is due to "user acceleration" versus gravitational
        acceleration, and it will report these parts separately.  However, the iPhone
        accelerometer is designed for making the UI rotate when the phone rotates, not
        for tracking user motion while being swung through the air.  If reports from
        other developers are to be believed, the iPhone configures its accelerometer for
        2 g's maximum acceleration.  This means that anything more than sedate walking
        motion will exceed the sensor's range.  If the sensor reports an out-of-range
        signal for a period of time, then we don't know the area under that part of the
        curve, and our integral will be off &mdash; and our double integral doubly so.  The
        way we will observe this is that swinging the iPhone and bringing it back to its
        starting position may produce what appears to the software to be a net change in
        velocity.  This net change in velocity leads to unbounded growth of the
        estimated position over time.</p>

        <p>This is not ideal.  From the perspective of control theory, what is happening
        is that the integrator is an unstable linear system: it does not exhibit the
        bounded-input, bounded-output property, because the integral of a finite
        constant taken over a suitably long interval will eventually exceed any bound.
        We need to stabilize the integrator, because users expect that the phone will
        not "fly off to infinity" in its own imagination and thus suffer loss of gesture
        recognition capability.  We can do this by moving poles on the S-plane.  If
        you're not into control theory, this will be the same as filtering out the DC
        component of the position and velocity, in order to prevent them from increasing
        forever.  Unfortunately, this technique leads to other funny behaviors,
        like the phone getting confused if a gesture takes too long.</p>

        <h3 id="motionBackground">Background</h3>
        <p>iOS provides device motion through the
        <a href="https://developer.apple.com/library/ios/documentation/CoreMotion/Reference/CMMotionManager_Class/" target="_blank"><code>CMMotionManager</code></a>,
        which is instantiated for you by the <code>Gesture3DViewController</code> like
        this:</p>

        <pre><code class="hljs objectivec">_motionManager = [[CMMotionManager alloc] init];
_motionManager.deviceMotionUpdateInterval = 1e-2;

CMDeviceMotionHandler handler = ^(CMDeviceMotion * _Nullable motion, NSError * _Nullable error) {
    [self accumulateMotion:motion];
};

CMAttitudeReferenceFrame referenceFrame = CMAttitudeReferenceFrameXArbitraryCorrectedZVertical;

[_motionManager startDeviceMotionUpdatesUsingReferenceFrame:referenceFrame
                                                    toQueue:[NSOperationQueue mainQueue]
                                                withHandler:handler];</code></pre>

        <p>This will result in periodic calls to Gesture3DViewController's
        <code>accumulateMotion:</code> method, which will be the subject of Tasks
        2A-2C.</p>

        <p>The contents of a
        <a href="https://developer.apple.com/library/ios/documentation/CoreMotion/Reference/CMDeviceMotion_Class/index.html#//apple_ref/occ/cl/CMDeviceMotion" target="_blank">device
        motion update</a> are attitude (the orientation of the device relative to the
        "reference frame"), rotation rate (rate of change of attitude), gravity
        (CoreMotion's best guess of the portion of accelerometer output which is due to
        the Earth), userAcceleration (the remainder of the accelerometer output), and
        magneticField (the external B field acting on the device).</p>

        <p>We will only be using attitude and userAcceleration; these are the fields we
        need to determine the device's movement in three-dimensional space.  iOS
        provides userAcceleration in instantaneous device coordinates:</p>

        <center><img id="accelerationAxes" src="images/acceleration_axes_2x.png" style="height:400px;"></center>

        <p>
        You can find this, and more information about the coordinate systems of the
        data, in
        <a href="https://developer.apple.com/library/ios/documentation/EventHandling/Conceptual/EventHandlingiPhoneOS/motion_event_basics/motion_event_basics.html#//apple_ref/doc/uid/TP40009541-CH6-SW27"
        target="_blank">Apple's documentation</a>.
        </p>

        <h3 id="rotations">Rotations and Orientations</h3>
        <p>In order to talk about devices that can not only move but also rotate through
        all different orientations in three dimensions, we need a quantitative way to
        talk about rotations and orientations.  We will consider rotations first.  If
        you are already familiar with this material, you are of course welcome to
        <a href="#backtoyou">skip down a bit</a>.</p>

        <h5>Rotations</h5>
        <p>Rotation is a physical <em>process</em> which proceeds continuously from a
        start point to an end point.  When we refer to rotations, however, we will think
        of them as operations, or functions, which map un-rotated inputs to rotated
        outputs, <em>skipping over any intermediate states</em>.  We refer to the
        "family" of rotations as the set of all such operations.  For instance, the
        "identity" rotation, I, has its output always exactly equal to its input.  You
        can think of this as a rotation by zero degrees.  The family of rotations
        includes a 90-degree rotation to the left, and an 89-degree rotation, and an
        89.999999-degree rotation.  Thus, the family of rotations is an infinite set,
        just as the real numbers are an infinite set.</p>

        <p>When you rotate your head, your eyes move relative to your surroundings.  In
        three dimensions, we distinguish between <em>translations</em>, which move every
        part of an object laterally while preserving its orientation, and
        <em>rotations</em>, which move each part of an object relative to some origin,
        but preserve the location of this origin.  Other types of transformations are
        possible in the real world, but rotations and translations are the only kinds
        which do not involve any amount of bending or stretching.  As an abstraction for
        physical objects which can be neither bent nor stretched, we refer to rotations
        and translations as the full set of "rigid body motions".  We could allow the
        origin of the rotation to vary, but this turns out to be no different than mixing
        translations and rotations, so we will fix the origin with no loss of
        generality.  You can think of it as the center of the phone.</p>

        <p>Rotations can be <em>composed</em> as functions.  Just as f(g(x)) refers to
        the output of f when its input is g(x), R<sub>2</sub>(R<sub>1</sub>(x)) is the
        output of rotation R<sub>2</sub> when acting on the result of R<sub>1</sub>
        acting on x.  With functions, we can use f(g(x)) = (f ∘ g)(x) to define a new
        function, f ∘ g, which takes inputs directly to outputs, skipping the
        intermediate step.  Similarly, we can consider R<sub>2</sub> ∘ R<sub>1</sub> as
        a new rotation, without any reference to x.  From now on, we will
        drop the "∘" symbol, and write simply R<sub>2</sub>R<sub>1</sub>.</p>

        <p>Just as functions do not, in general, have the property f(g(x)) = g(f(x)), as
        we can check by comparing e.g. the graphs of sin(x<sup>2</sup>) and
        sin<sup>2</sup>(x), so rotations do not in general satisfy
        R<sub>1</sub>R<sub>2</sub> = R<sub>2</sub>R<sub>1</sub>.  There are special
        cases, but you may find it helpful at this point to file away the fact that
        general rotations are not <a href="https://en.wikipedia.org/wiki/Commutative_property" target="_blank">commutative</a>.</p>

        <p>Note that composing any rotation R with the identity I gives the same
        rotation that we started with: whether we compose them as RI or IR, we have
        RI=IR=R, because I returns its input unchanged.  This is one of those special
        cases we mentioned: I commutes with any rotation.</p>

        <p>Rotations can also be <em>inverted</em> as functions.  R<sup>-1</sup>(R(x))
        is identically x.  We can therefore define inversion by requiring <span
        class="nowrap">R<sup>-1</sup> R = I</span>.  On physical grounds,
        it is clear that the inverse of a rotation is also a rotation, because we can
        always restore a rotated object back to its starting orientation without
        bending, stretching, or translating it.  It's also good to note that composition
        of rotations is <em>associative</em>; that is,
        (R<sub>1</sub>R<sub>2</sub>)R<sub>3</sub> =
        R<sub>1</sub>(R<sub>2</sub>R<sub>3</sub>).</p>

        <p>We have now encountered four properties of the set of rotations under
        composition &mdash; closure (the fact that the composition of two rotations is a
        rotation); associativity; identity (the existence of a rotation I that leaves
        any other rotation unchanged under composition); and the existence of an inverse
        rotation for any given rotation.  These are the defining properties of a
        <a href="https://en.wikipedia.org/wiki/Group_(mathematics)" target="_blank">group</a>
        structure; from now on, we will talk about the group of rotations instead of the
        set or family, to emphasize that rotations obey these particular algebraic laws.
        Occasionally, in other contexts, you will hear the group of rotations in three
        dimensions referred to as SO(3), or the special orthogonal group.</p>

        <p>We've stated that the group of rotations consists of transformations which
        neither bend nor stretch an object, and leave the origin unchanged.  An
        additional property of each individual rotation is that it must leave not just
        one, but infinitely many points unchanged along a line through the origin.  This
        line is called the axis of the rotation, and the plane at right angles to this
        line is called the plane of rotation.</p>

        <div class="aside">
            <h6>Aside:</h6>
            <p>In introductory treatments of rotations, it's common to see emphasis
            placed on the axis of rotation, and on constructing e.g. the rotation that
            operates about a given axis by a given angle.  It turns out, though, that
            this notion of rotating about an axis is not fundamental to rotations, but
            rather to the three-dimensional space in which we live.  To see this,
            consider rotations in two dimensions by an angle φ.  In polar coordinates,
            these transformations map points (r,θ) to (r,θ+φ).</p>

            <p>If we try to draw the axis of this rotation, we will be forced to draw a
            line perpendicular to the two-dimensional plane we're working in.  We can do
            that, because we're three-dimensional.  But that's cheating, because it
            introduces three dimensions in an attempt to explain a fundamentally
            two-dimensional concept.  Really, there are zero lines in two dimensions
            perpendicular to the plane of rotation.  The problem is worse in four
            dimensions, where rather than zero axes of rotation, we have infinitely
            many.</p>

            <p>The more general concept is the plane of rotation, which is meaningful in
            any space of dimension two or more.  So instead of a rotation about the z
            axis, we'll be persnickety and talk about a rotation in the x-y plane.</p>
        </div>

        <p>From geometry, we know that it is always possible to break apart a vector
        in three dimensions into a component in the plane of rotation and a component
        perpendicular to the plane.  We can understand the action of a rotation on a
        vector by allowing it to act on each of these two component vectors separately,
        and then adding the results back together.</p>

        <p>For the component in the plane, we can see the behavior very simply in polar
        coordinates.  The radial coordinate of a point, measured relative to the origin,
        should be unchanged by a rotation, and the angular coordinate should increase by
        the angle of the rotation.  We can therefore write the action of the rotation on
        the component in the plane as <span class="nowrap">(r,θ) → (r',θ') = (r,θ+φ)</span>.</p>

        <p>On the component perpendicular to the plane, as we have mentioned, the
        rotation does not act at all.  So we can write the action of the rotation on
        this component as Z → Z' = Z.</p>

        <h5>Representing Rotations Numerically</h5>
        <p>If rotations had no further properties than those of functions in general,
        we'd have no hope of representing all of them in a finite amount of memory.
        Fortunately, the group of rotations is extremely restrictive compared to general
        functions.  Based on the preceding discussion, we can immediately state that
        given the plane of rotation (specified, for instance, via its normal vector) and
        the angle of the rotation (where we have to be careful to define which direction
        is plus and which is minus), the rotation's action on all points, in and out of
        the plane, is fully determined: on the vector component perpendicular to the
        plane, the rotation does nothing; and on the component in the plane, the
        rotation alters the polar coordinates of the point by an angle addition.  So we
        don't need more than four numbers (normal vector plus angle) to specify a
        rotation.</p>

        <p>In fact, observing that the normal vector always has unit length, and that
        negating both the normal vector and the angle of the rotation gives the same
        rotation as before, we can be even more concise.  We can specify just the three
        components of a vector whose length is the absolute value of the angle and whose
        direction specifies the (oriented!) axis of rotation.  The action of such a
        rotation on any vector can be evaluated using <a
        href="https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula" target="_blank">a little bit
        of vector algebra</a>.  For those who want to chase this rabbit a little
        further, you are welcome to read up on the <a
        href="https://en.wikipedia.org/wiki/Exponential_map_(Lie_theory)" target="_blank">exponential
        map</a>, which expresses the relationship between this axis-angle representation
        and the group of rotations.  You are doubly-welcome to read up on spherical
        linear interpolation, which goes by the (charming) name <a
        href="https://en.wikipedia.org/wiki/Slerp" target="_blank">"slerp"</a>.</p>

        <p>There are many other formalisms for representing the group of rotations,
        including 3×3 matrices of determinant 1 with the property that their columns
        (respectively, rows) are mutually-orthogonal and of unit length; Euler angles;
        quaternions; and
        <a href="https://en.wikipedia.org/wiki/Rotation_formalisms_in_three_dimensions" target="_blank">so forth</a>.
        One reason for the proliferation of alternatives in this space is that for any
        given choice, it is usually either painful to compute the action of the rotation
        on a vector, or it is painful to compose rotations together into a new rotation,
        or it is painful to keep track of the accumulation of many small rotations over
        time due to numerical round-off errors, or it is painful to invert a rotation.
        Or, if you're very lucky, there are situations where the coordinates "seize up"
        in the sense that nearby rotations no longer have nearby coordinates (see <a
        href="https://en.wikipedia.org/wiki/Gimbal_lock" target="_blank">gimbal lock</a>).</p>

        <p>For this lab, you will be using
        <a href="https://en.wikipedia.org/wiki/Quaternion" target="_blank">quaternions</a>.
        Quaternions share many properties with complex numbers, and can be thought of as
        a real number plus an imaginary vector.  These objects therefore have four
        components.  Rotations are specifically represented by quaternions whose squared
        components sum to 1, the so-called unit quaternions.  These are related to the
        axis-angle representation discussed earlier by</p>

<pre>q<sub>w</sub> = cos(φ/2)
q<sub>x</sub> = n<sub>x</sub> sin(φ/2)
q<sub>y</sub> = n<sub>y</sub> sin(φ/2)
q<sub>z</sub> = n<sub>z</sub> sin(φ/2),</pre>

        <p>where the vector n is normal to the plane of rotation, and φ is the angle of
        rotation.  The rules for multiplying quaternions by quaternions and applying
        quaternions to vectors are somewhat tedious; fortunately, you can call <a
        href="#GLKQuaternionRotateVector3">subroutines</a> for that.</p>

        <h5>Orientations</h5>
        <p>Because there is no notion of absolute orientation in the laws of physics, we
        always measure an object's orientation <em>relative</em> to some "standard"
        orientation.  In our case, we specified the standard orientation when we passed
        the constant
        <a href="https://developer.apple.com/library/prerelease/ios/documentation/CoreMotion/Reference/CMAttitude_Class/index.html#//apple_ref/c/econst/CMAttitudeReferenceFrameXArbitraryCorrectedZVertical" target="_blank"><code>CMAttitudeReferenceFrameXArbitraryCorrectedZVertical</code></a>
        to CoreMotion.  The orientation of an object, then, can be represented by the
        rotation which translates vectors from the object's (instantaneous) local
        coordinate system to the corresponding vectors in the (persistent) coordinate
        system of a hypothetical copy of the object in standard orientation.  This
        rotation is precisely the value returned by the CMDeviceMotion's
        <code>attitude</code> property.</p>

        <h3 id="backtoyou">Responsibilities of <code>accumulateMotion:</code></h3>
        <p><code>accumulateMotion:</code> will need to convert the data from iOS into a
        consistent set of coordinates, track the user's motion over time, and hand off
        the data to be stored and eventually transmitted to the
        <code>GestureProcessor</code>.  Here's the outline of
        <code>accumulateMotion:</code> that you will start with, which you can find in
        Gesture3DViewController.m under InertialMotion/User Interface in Xcode's project
        navigator.</p>

        <pre id="accumulateMotion"><code class="hljs objectivec">- (void)accumulateMotion:(CMDeviceMotion *)motion
{
    double dt = _motionManager.deviceMotionUpdateInterval;
    GLKQuaternion attitude = GLKQuaternionFromCMQuaternion(motion.attitude.quaternion);
    GLKVector3 userAcceleration = GLKVector3FromCMAcceleration(motion.userAcceleration);
    
    // -- <span id="task2a_code">TASK 2A</span> --
    GLKVector3 acceleration = userAcceleration;
    // rotate acceleration from instantaneous coordinates into persistent coordinates
    
    // -- <span id="task2b_code">TASK 2B</span> --
    // integrate acceleration into _velocity and _velocity into _position
    
    // -- <span id="task2c_code">TASK 2C</span> --
    // apply your choice of braking to _velocity and _position to stabilize the integration loop
    
    // add the new data to the log
    [self appendPoint:_position attitude:attitude];
}</code></pre>

        <p><code>appendPoint:attitude:</code> will store the data while the user is
        touching the screen, and when the user stops touching the screen, it will call
        the <code>GestureProcessor</code>'s
        <code>processGesture3DWithSamples:count:minSize:</code> method, which will be
        the subject of Section 3.</p>

        <div class="aside">
            <h6>Aside:</h6>
            <p>GLK is short for GLKit, which is an Apple framework (library)
            intended to make OpenGL programming easier for 2-D and 3-D
            graphics.  We're using it here and elsewhere because it provides a
            fairly complete set of vector/matrix manipulation functions.  We've
            <a href="#appendix">catalogued</a> a few of these functions for
            you, but you can find the full set through
            <a href="https://developer.apple.com/library/ios/documentation/GLkit/Reference/GLKit_Collection/#other" target="_blank">Apple's documentation</a>.</p>
        </div>

        <h3 id="task2a">Task 2A &mdash; Coordinate Transformation</h3>
        <p>Your first task in <a href="#task2a_code"><code>accumulateMotion:</code></a>
        will be to use the quaternion stored in <code>attitude</code> to convert the
        vector <code>acceleration</code> into a persistent set of coordinates that do
        not vary with the orientation of the phone.  Fortunately, there's a function in
        the <a href="#appendix">appendix</a> which will do this for you in one step.
        Yes, we're going to make you hunt for it.  You'll know it when you see it.  Note
        well that none of these functions modifies the variable you pass in; they return
        new objects, which you will have to assign back to the variable you wish to
        modify.</p>

        <p>For reasons which would totally be hilarious if we knew what they were, you
        will also want to negate <code>acceleration</code> at this point (otherwise the
        visualization will look wrong later).  It does not matter whether you do this
        before or after applying the rotation.  You can negate all the components of
        <code>acceleration</code> manually (you'll have to look up the names of the
        components of the <code>GLKVector3</code> structure in the headers, for instance
        by command-clicking on "GLKVector3" somewhere that it appears in the code), or
        you can use the function listed in the appendix that multiplies a vector by a
        scalar to multiply by -1.0.</p>

        <div class="aside">
            <h6>Aside:</h6>
            <p>Note that we could have hidden this step from you by negating the vector
            somewhere else in the app.  But you may find that no matter how careful you
            are in working with coordinate systems in mobile programming, sooner or
            later you will encounter a minus sign or something similar that you can't
            track down.  You can either spend a lot of time reasoning about how all the
            components of your app work together to produce the issue, or you can fix it
            in an <em>ad hoc</em> way like we are doing here.  Unfortunately, if you use
            more than one or two such fixes, you can quickly get into a situation where
            there are 2<sup><em>n</em></sup> possible ways to flip all the unexplained bits in
            various parts of your program, and you can't find an assignment that works.
            This is an example of technical debt.  When you're working on a larger
            project than this simple app, you'll have to factor that future possibility
            into your decision to make a quick fix versus tracking down the real
            problem.</p>
            
            <p>Extra credit is available to anyone who can explain to our satisfaction
            why there is a minus sign here.</p>
        </div>

        <h3 id="task2b">Task 2B &mdash; Integration</h3>

        <p>We can write down a set of differential equations for the position and
        velocity in terms of the acceleration as follows:</p>
        <div><div style="width:200px; display:inline-block;"><script type="math/tex; mode=display">\frac{d\mathbf v}{dt}=\mathbf a,</script></div>
        <div style="width:200px; display:inline-block;"><script type="math/tex; mode=display">\frac{d\mathbf x}{dt}=\mathbf v</script></div></div>

        <p>where we use bold to denote vector quantities.  A simple and inaccurate (but
        often acceptable) way to solve these equations is the
        <a href="https://en.wikipedia.org/wiki/Euler_method" target="_blank">Euler
        method</a>.  To apply this method, we replace the infinitesimal quantities in
        the differential equations with finite differences, and rearrange:
        <div><div style="width:200px; display:inline-block;"><script type="math/tex; mode=display">\Delta\mathbf v=\mathbf a\Delta t,</script></div>
        <div style="width:200px; display:inline-block;"><script type="math/tex; mode=display">\Delta\mathbf x=\mathbf v\Delta t</script></div>
        <p>and hence</p>
        <div style="width:400px;"><script type="math/tex; mode=display">\Delta t = t_i - t_{i-1}</script></div>
        <div style="width:400px;"><script type="math/tex; mode=display">\mathbf v(t_i) = \mathbf v(t_{i-1}) + \mathbf a(t_i) \Delta t</script></div>
        <div style="width:400px;"><script type="math/tex; mode=display">\mathbf x(t_i) = \mathbf x(t_{i-1}) + \mathbf v(t_i) \Delta t</script></div>

        <p>Actually, it would be more accurate to split the velocity update into two
        parts, because a constant acceleration acting for time <script type="math/tex;
        mode=inline">\Delta t</script> should give a change in position of <script
        type="math/tex; mode=inline">\frac12(\Delta t)^2\mathbf a</script>.  You can
        check for yourself that the method above erroneously yields a change in position
        two times larger.</p>
        <div style="width:400px;"><script type="math/tex; mode=display">\mathbf v'(t_i) = \mathbf v(t_{i-1}) + \frac12\mathbf a(t_i) \Delta t</script></div>
        <div style="width:400px;"><script type="math/tex; mode=display">\mathbf x(t_i) = \mathbf x(t_{i-1}) + \mathbf v'(t_i) \Delta t</script></div>
        <div style="width:400px;"><script type="math/tex; mode=display">\mathbf v(t_i) = \mathbf v'(t_i) + \frac12\mathbf a(t_i) \Delta t</script></div>
        <p>The procedure outlined here has allowed us to take a continuous-time system
        of (ordinary) differential equations and turn it into a discrete-time
        integration rule using a small, finite number of additions and multiplications.
        This is just a glimpse of a
        <a href="https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations" target="_blank">rich</a>
        <a href="https://en.wikipedia.org/wiki/Numerical_partial_differential_equations" target="_blank">field</a>.

        <p>Your job for this task is to translate this math into code in
        <a href="#task2b_code"><code>accumulateMotion:</code></a>.  The
        <code>Gesture3DViewController</code> already has instance variables called
        <code>_velocity</code> and <code>_position</code> that you should use for this
        purpose, and we've initialized a variable <code>dt</code> with the expected time
        between updates.  You can change the code to set <code>dt</code> to the actual
        time between updates if you wish, using e.g. motion.timestamp.  If you choose to
        do this, you'll have to introduce a new instance variable to hold the last
        timestamp, and you'll have to include logic for doing the right thing when the
        first sample comes in.  The vector manipulation functions in the <a
        href="#appendix">appendix</a> are likely to be useful for this task.</p>

        <h3 id="task2c">Task 2C &mdash; Stabilization</h3>
        <p>As we've discussed, an integrator like the linear system you implemented in
        task 2B is unstable in the sense that a bounded input acceleration can lead to
        an unbounded output position estimate.  In fact, for a constant nonzero input,
        we would expect to see quadratic growth in the output position.  Among the
        undesirable properties of such a system is the way that small errors in the
        acceleration lead to large errors in the estimated position.  You need to tweak
        your code from task 2B to stabilize the system.  Feel free to adopt any of the
        following approaches, or to develop your own.</p>

        <ul>
            <li>Exponential damping.  By artificially reducing the velocity and position
            estimates towards zero at each step, with a rate proportional to their
            current estimates, we effectively kill the quadratic growth of errors by
            imposing an exponential decay.  This corresponds to changing the initial
            linear system to
            <div><div style="width:200px; display:inline-block;"><script type="math/tex; mode=display">\frac{d\mathbf v}{dt}=\mathbf a - \alpha \mathbf v</script></div>
            <div style="width:200px; display:inline-block;"><script type="math/tex; mode=display">\frac{d\mathbf x}{dt}=\mathbf v - \alpha \mathbf x</script></div></div>
            or making the (approximately) equivalent change to the Euler update
            equations,
            <div style="width:400px;"><script type="math/tex; mode=display">\mathbf v'(t_i) = e^{-\alpha \Delta t/2} \mathbf v(t_{i-1}) + \frac12\mathbf a(t_i) \Delta t</script></div>
            <div style="width:400px;"><script type="math/tex; mode=display">\mathbf x(t_i) = e^{-\alpha \Delta t} \mathbf x(t_{i-1}) + \mathbf v'(t_i) \Delta t</script></div>
            <div style="width:400px;"><script type="math/tex; mode=display">\mathbf v(t_i) = e^{-\alpha \Delta t/2} \mathbf v'(t_i) + \frac12\mathbf a(t_i) \Delta t</script></div>

            <li>Nonlinear damping.  Rather than tweaking the linear system, you could
            directly alter its state in a non-linear way; for instance, you could set
            the velocity and position to zero whenever the user lets go of the screen,
            or you could reduce the magnitude of these vectors by a constant amount per
            unit time, e.g. <script type="math/tex; mode=inline">\mathbf x \rightarrow
            \mathbf x \cdot\mathrm{max}(\frac{|\mathbf x| -\text{step}}{|\mathbf x|},
            0)</script>, or you could choose some other rule.

            <li>Rest recognition.  If the acceleration doesn't fluctuate by more than a
            certain amount (pick some threshold value) for a certain period of time, you
            could conclude that the phone is stationary.  For flair, you could even go
            back through your collected and yet-to-be-processed data, and apply a
            correction to the modeled positions based on your newfound knowledge of the
            phone's velocity.
        </ul>

        <p>If you choose to develop your own approach, keep in mind that data is always
        noisy and models are always incomplete.  You want your system to be "robust" in
        the sense that the behavior of the app right now depends little or not-at-all on
        questionable input from the distant past.  That is, think of bad data as water
        under the bridge.  Don't let its effects on your program's state linger
        forever.</p>

        <p>At this point, you should be able to draw ribbons in the air by tabbing over
        to the 3D screen and moving/rotating the phone while touching the screen.  We
        will check that you have stabilized the integration loop by waving the phone
        vigorously in the air, and then watching to see if the computed position flies
        off to infinity or stays finite.  It's fine if your app needs a few moments to
        recover.</p>

        <p>You should be able to make recognizable 3-D shapes, like squares, triangles,
        circles, checkmarks, and x's, using your app.  If you cannot draw shapes, check
        your work and come talk with us.</p>

        <h3 id="task2d">Task 2D &mdash; Optional Improvements</h3>
        <p>The acceleration data provided by CoreMotion seems to have some start-up
        transients, which can throw off your integration for a long time after start-up.
        You are welcome to invent a solution for this problem.</p>

        <h2 id="sec3">Section 3 &mdash; Mapping 3-D Gestures to 2-D</h2>
        <p>For this section, you will reduce the 3-D gesture recognition problem to the 2-D
        case.  With any luck, you'll soon be recognizing letters formed by waving the
        phone in the air while touching the screen.</p>

        <p>You will be working on
        <code>processGesture3DWithSamples:count:minSize:</code>, which you can
        find in GestureProcessor.m.  Here's the initial skeleton.</p>

        <pre id="process3d"><code>- (void)processGesture3DWithSamples:(const Sample3D *)samples
                              count:(NSUInteger)count
                            minSize:(double)minSize {
    Sample2D samples2D[count];
    
    // -- <span id="task3a_code">TASK 3A</span> --
    // Estimate left-right, up-down axes by averaging orientation over time:
    GLKMatrix3 M = {};
    // For each i, convert samples[i].attitude to a 3x3 matrix and sum it into M.
    // Then find the rotation matrix most similar to the resulting sum.
    
    // -- <span id="task3b_code">TASK 3B</span> --
    // Project points to 2D:
    // For each i, form the matrix-vector product of M with samples[i].location
    // and copy the transformed x and y coordinates, along with the timestamp,
    // to samples2D[i].
    
    // Apply 2-D solution
    [self processGesture2DWithSamples:samples2D count:count minSize:minSize];
}</code></pre>

        <h3 id="task3a">Task 3A &mdash; Estimate Axes</h3>
        <p>When the user makes a gesture in the air with their phone, they have a mental
        picture of whether they are moving the phone left or right, up or down.  For
        instance, if the user holds the phone out in front of them, then up-and-down
        gesturing will correspond to motion in the vertical dimension.  If, however, the
        user is hunched over the phone (and hence is looking downward towards the
        screen), then the phone's long axis, and the user's notion of up-and-down
        gesturing relative to the screen, will be horizontal.</p>

        <center><img src="images/gesturedirection1500.jpg" style="width:750px;"></center>

        <p>Since the user is free to change posture (and hence the orientation of the
        phone) at any time, but is not likely to do so in the middle of a gesture, we
        can average the orientation of the device during the gesture to guess what the
        user is currently thinking of as the up-down and left-right axes.</p>
        
        <p>We also recall the definition of the instantaneous device coordinate system:</p>

        <center><img id="accelerationAxes" src="images/acceleration_axes_2x.png" style="height:400px;"></center>

        <p>Your job for this part is to use the attitude (that is, orientation) quaternion
        recorded in each of the 3-D motion samples in
        <a href="#task3a_code"><code>processGesture3DWithSamples:count:minSize:</code></a>
        to estimate the instantaneous orientation of the phone's y-axis &mdash; a proxy
        for the user's notion of up and down &mdash; and the phone's x-axis &mdash; a
        proxy for the user's notion of left and right.</p>

        <p>What we are doing here is to determine the average of the phone's orientation
        over the period of the gesture.  However, what it means geometrically to average
        orientations together is a bit ambiguous.  To visualize this, suppose that we
        were averaging orientations (rotations) in two dimensions.  What is the average
        of a 359-degree rotation and a zero-degree rotation?  It's certainly not a
        179.5-degree rotation, which is farther from either of the two inputs than they
        are from each other.  We could say that the average is a 359.5-degree rotation,
        but then what is the average of a 90-degree and a 270-degree rotation?  Or what
        is the average of a group of four rotations by angles 0, 90, 180, 270
        degrees?</p>

        <p>A
        <a href="http://epubs.siam.org/doi/abs/10.1137/S0895479801383877" target= "_blank">rigorous treatment of this issue</a>
        is available, invoking some heavy-duty linear algebra which we will not require
        you to study.  Instead, we've provided you with a function,
        <code>NearestRotation</code> in Geometry.m, that will call into iOS's
        implementation of the Netlib Linear Algebra PACKage (LAPACK), an exceedingly
        popular and well-maintained library.  LAPACK is, unfortunately, written in
        Fortran with an interface consisting of six-character function names like
        <a href="http://www.netlib.org/lapack/explore-html/d1/d7e/group__double_g_esing.html#gad8e0f1c83a78d3d4858eaaa88a1c5ab1" target="_blank"><code>dgesdd</code></a>.
        We wouldn't deny you the pleasure of reading the code behind that marvel, or of
        studying the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition" target= "_blank">
        technique it implements</a>.  For our purposes, what you need to know is that
        the average of a set of rotations can be taken to mean the result of a
        three-step procedure:</p>

        <ol>
            <li>Convert the quaternion representing the orientation of each sample (that
            is, samples[i].attitude) to a 3×3 matrix representing the rotation.
            <li>Average together the resulting matrices, each matrix component
            separately.  (note: the average of rotation matrices is not, in general, a
            rotation matrix.)
            <li>Find the rotation matrix closest to the average matrix.
        </ol>

        <p>The final result can be converted back into a quaternion, or used directly in
        matrix form.  Step 3 is what <code>NearestRotation</code> does.</p>

        <h3 id="task3b">Task 3B &mdash; Project Coordinates</h3>
        <p>The final task for this lab is to project the coordinates of the 3-D sample
        points to 2-D coordinates suitable for use by
        <code>processGesture2DWithSamples:count:minSize:</code>.
        In the <a href="#task3b_code">appropriate place</a> in the code, you
        will need to form the matrix-vector product of the rotation matrix M
        from Task 3A with <code>samples[i].location</code> for each i, and then
        copy the transformed x and y coordinates, along with the timestamp
        <code>samples[i].t</code>, into <code>samples2D[i]</code>.</p>

        <h3 id="task3c">Task 3C &mdash; Optional Improvements</h3>
        <p>The gesture recognition results that we have obtained using the procedure we
        have outlined for you have not been fantastic.  One reason for that is the poor
        fit between the sensor data (acceleration) and the machine learning features
        (position, motion, and dwell time).  We would like to encourage you to think
        about an alternative set of machine learning features which are a more direct
        fit to the 3-D motion problem and the accelerometer data.  Let us know what you
        discover!</p>

        <h2 id="appendix">Appendix: Functions</h2>
        <p>You may find the following functions, macros, and data types useful for the
        exercises in this lab.  It's better to use system-provided functions and data
        types rather than defining your own because the result will (usually) be more
        concise, readable, performant code.</p>

        <center>
            <table><tr><th>Name</th><th>Description</th></tr>
                <tr><td><code>MIN(a, b)</code></td><td>This is actually a preprocessor macro that works on any data type.  It's smart enough to only evaluate its arguments once,
                                           which is not true of the naïve way of writing such a macro, <code>(((a) < (b)) ? (a) : (b))</code>, which might evaluate each argument
                                           twice if the compiler can't convince itself that they are side-effect free.</td></tr>
                <tr><td><code>MAX(a, b)</code></td><td>Same story.  Contrast these macros with <code>fmin</code>/<code>fmax</code>/<code>fminf</code>/<code>fmaxf</code>, which require the programmer to use a different function for each data type.</td></tr>
                <tr><td><code>exp(x)</code></td><td>Returns the double <span style="font-style: italic;">e<sup>x</sup></span>.</td></tr>
                <tr><td><code>[NSDate timeIntervalSinceReferenceDate]</code></td>
                    <td>Returns number of seconds (including fractional part) since a particular fixed date in 2001.  Note that the
                        return value is a double, a 64-bit floating-point (e.g.  scientific notation) data type.  Of those 64 bits, a
                        certain number are used for the exponent, so in effect, a double has 53 significant bits (analogous to
                        significant figures in base 10), and when used to store the time interval since the reference date, gives a
                        resolution of &pm;26 nanoseconds.  However, if you cast the double returned by this function to a float (32 bits)
                        or store it in a variable of type float, you will lose bits, leaving only 24 significant bits.  This leaves
                        you with a resolution of &pm;14.1 <em>seconds</em> &mdash; so don't use float to represent time intervals; stick with
                        double.</td></tr>
                <tr><td><code>GLKVector3</code></td><td>A three-dimensional vector data type.  Its members can be accessed with .x, .y, .z.</td></tr>
                <tr><td><code>GLKVector3Make(x, y, z)</code></td><td>A helper function which returns a GLKVector3 with the specified coordinates.</td></tr>
                <tr><td><code>GLKQuaternion</code></td><td>A four-dimensional vector data type useful for representing rotations in three dimensions.</td></tr>
                <tr><td><code>GLKVector3Add(a, b)</code></td><td>Returns the element-wise sum of two vectors.</td></tr>
                <tr><td><code>GLKVector3Subtract(a, b)</code></td><td>Returns the element-wise difference of two vectors.</td></tr>
                <tr><td><code>GLKVector3MultiplyScalar(a, b)</code></td><td>Returns a vector whose components are the scalar b times the components of a.</td></tr>
                <tr><td><code>GLKVector3Normalize(a)</code></td><td>Returns a / |a|.</td></tr>
                <tr><td><code id="GLKQuaternionRotateVector3">GLKQuaternionRotateVector3(q, a)</code></td><td>Returns the result of applying the rotation represented by q to a.</td></tr>
                <tr><td><code>GLKVector3DotProduct(a, b)</code></td><td>Returns the sum over components of the element-wise product of a and b.</td></tr>
                <tr><td><code>GLKMatrix3</code></td><td>A 3×3 matrix data type.  See its definition for details.</td></tr>
                <tr><td><code>GLKMatrix3MakeWithRows</code></td><td>Accepts three vectors and returns a 3×3 matrix.</td></tr>
                <tr><td><code>GLKMatrix3MultiplyVector3(M, a)</code></td><td>Returns the result of a matrix-vector product.</td></tr>
                <tr><td><code>GLKQuaternionFromCMQuaternion(motion.attitude.quaternion)</code></td><td>Our helper function which converts between CoreMotion's rotation data type and GLKit's.</td></tr>
                <tr><td><code>GLKVector3FromCMAcceleration(motion.userAcceleration)</code></td><td>Our helper function which converts between CoreMotion's acceleration data type and GLKit's vector data type.</td></tr>
            </table>
        </section>
    </div>
    <script type="text/javascript">
    window.MathJax={showProcessingMessages:false}
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full"></script>
</body>
</html>
