<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<html>

<head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="6.S062 : Mobile and Sensor Computing Course">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.1.0/styles/default.min.css">
    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">
    <title>6.S062 - Semester Project Presentations</title>
    <style type="text/css">
        table.projects {
            margin-left:auto; 
            margin-right:auto;
            width:80%;
            max-width:940px;
            border:none;
        }

        table.projects td.title {
            padding-right:50px;
            width:500px;
            border:none;
            padding-bottom:10px;
        }

        table.projects td.title img {
            width:50px;
            border:none;
            box-shadow:none;
            display:block;
            float:left;
            margin-left:-68px;
            margin-right:18px;
            padding-left:0;
            padding-right:0;
        }

        table.projects td.left {
            padding-left:10px;
            padding-right:36px;
            padding-top:0;
            margin-top:0;
            width:500px;
            border:none;
            text-align:justify;
            vertical-align:top;
        }

        table.projects td.left p {
            margin-top:0;
        }

        table.projects td.right {
            padding:0;
            margin:0;
            padding-top:5px;
            max-width:400px;
            border:none;
        }

        table.projects td.right img {
            width:100%;
            max-width:400px;
            min-width:200px;
            padding:0;
            margin:0;
            border:none;
            box-shadow:none;
            display:block;
        }

        table.projects td.buttons {
            border:none;
            padding:0;
            text-align:right;
            max-width:400px;
            vertical-align:middle;
        }

        table.projects td.buttons a {
            vertical-align:middle;
            text-decoration:none;
            cursor:pointer;
            display:inline-block;
            width:60px;
            height:50px;
            padding:0;
            margin:0;
            margin-left:6px;
            background-color:#bbbbc2;
            text-align:center;
            transition: all 0.25s ease-in;
            color:rgba(0,0,0,0);
            background-image:none;
            position:relative;
            line-height:20px;
            padding-top:10px;
        }

        table.projects td.buttons a:hover {
            color: #000;
        }

        table.projects td.buttons a img {
            opacity:1.0;
            transition: all 0.25s ease-in;
            box-shadow:none;
            border:none;
            padding:0;
            margin:0;
            position:absolute;
        }

        img.repo {
            width:51px;
            height:55px;
            left:6px;
            top:5px;
        }

        img.pdf {
            width:42px;
            height:40px;
            left:10px;
            top:10px;
        }

        img.video {
            width:42px;
            height:40px;
            left:10px;
            top:10px;
        }

        img.data {
            width:42px;
            height:41px;
            left:9px;
            top:10px;
        }

        table.projects td.buttons a:hover img {
            opacity:0.0;
        }
    </style>
</head>
<body>
    <div id="header_wrap" class="outer">
        <section id="main_content" class="inner">
            <header class="inner">
                <h1 id="project_title">Semester Project Presentations</h1>
                <h2 id="project_tagline">6.S062 Mobile and Sensor Computing</h2>
                <p style="color:#fff">Wednesday, May 11, 11am-12:30pm, 32-G9 Lounge</p>
            </header>
        </section>
    </div>
    <div id="main_content_wrap" class="outer">
        <section id="main_content" class="inner">
            <p>
            On Wednesday, May 11, from 11am-12:30pm, the students of 6.S062
            (Mobile and Sensor Computing) presented posters and demos of
            their semester projects in the G9 lounge.
            </p>

            <hr>
        </section>

            <table class="projects">
                <tr>
                    <td class="title">
                        <img src="images/noun_251228_cc.svg">
                        <h5>Activity Recognition and Correction for Weight Lifting</h5>
                        <h6>Katie Bartel, Kwame Efah</h6>
                    </td>
                    <td class="buttons">
                        <a target="_blank" href="posters/bartel-efah.pdf">Poster<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://github.com/kwbartel/BenchBuddy">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td>
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        Weight lifting is an increasingly popular hobby. A key component to improving
                        one's strength through weight lifting is meticulous record keeping of each
                        workout. Additionally, many novice weight lifters injure themselves with poor
                        form. We explore the potential of data collected from inertial measurement
                        units (accelerometers and gyroscopes) fixed to a person's forearms to perform
                        automatic record keeping of workouts, including: 1) recognition what weight
                        lifting exercise a user has performed 2) counting of the number of repetitions
                        performed 3) feedback on the user's form. We developed a pair of wearable,
                        bluetooth capable sensor modules and a companion iOS app which performs the
                        activity recognition, repetition counting, and form feedback. Combining the
                        power of machine learning, sensor data, and mobile devices, our system makes
                        weight lifting more accessible and safe for everyone.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_0663.jpg">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <img src="images/noun_521.svg">
                        <h5>Verge, Estimated Time of Arrival for Pedestrian Commutes</h5>
                        <h6>Jorrie Brettin, Brendan Chang</h6>
                    </td>
                    <td class="buttons">
                        <a target="_blank" href="posters/brettin-chang.pdf">Poster<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://github.com/BrendanSChang/verge">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td>
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        As students, we have many responsibilities&mdash;so many that we inevitably find
                        ourselves asleep well past the time we were supposed to wake up, late to class
                        or an important meeting. In the process of rushing to your commitment in
                        yesterday's clothes with unkempt hair, it would be nice to have an accurate
                        estimate of your time of arrival. We developed an app which uses a combination
                        of localization methods to give the user an accurate estimated arrival time to
                        their destination. For the outdoor case, we use GPS to provide the user with
                        the highest location precision available. When indoors, GPS doesnâ€™t work;
                        instead, we use a combination of dead reckoning and magnetic headings to
                        estimate user trajectory. By integrating the two location services, we intend
                        to give an accurate time estimate to users entering and exiting buildings on
                        their way to a destination.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_0670.jpg">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <img src="images/noun_110451_cc.svg">
                        <h5>Gesture-Encrypted Location-Based Messaging</h5>
                        <h6>Danny Callahan, Andrew Esquivel, Ryan Frankel, Harrison Wang</h6>
                    </td>
                    <td class="buttons">
                        <a target="_blank" href="posters/callahan-esquivel-frankel-wang.pdf">Poster<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://github.mit.edu/abe707/6mob-final">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                        <a target="_blank" href="https://github.mit.edu/abe707/6mob-final-data-collection">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                        <a target="_blank" href="https://github.mit.edu/abe707/6mob-final-server">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td>
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        Many current existing applications allow individuals to post to message boards
                        based on location.  Though these solutions allow large quantities of people to
                        communicate about activities or events ongoing in their location, they are
                        largely anonymous in nature, cover relatively large areas, and do not
                        necessarily allow individuals to communicate on a more personal basis.  For
                        these reasons, we developed a location-based, gesture-encrypted messaging
                        application to allow individuals to send one another more targeted messages
                        based on their locations.  A user can send a message that is encrypted with a
                        gesture.  We chose to explore gesture encryption techniques to further
                        understand how a gesture security system could be implemented. When another
                        user comes within the proximity of this location, he or she is prompted to
                        match the gesture.  We break gestures into feature vectors, and perform fuzzy
                        matching to determine whether or not the gestures match.  In the event of a
                        matched gesture, the message is released for the user to view.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_0669.jpg">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <img src="images/noun_75728_cc.svg">
                        <h5>The MIT Tour-ing Machine</h5>
                        <h6>Natasha Consul, Smriti Pramanick</h6>
                    </td>
                    <td class="buttons">
                        <a target="_blank" href="posters/consul-pramanick.pdf">Slides<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://github.com/smritip/mit-touring-machine">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td>
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        Our project provides those that frequent MIT (students, tourists, etc.) an
                        interactive and informative tool to learn about MITâ€™s rich history, such as
                        hacks. We designed and developed an iOS application focusing on a tour of the
                        Stata Center. The application provides a &ldquo;museum tour&rdquo; highlighting some major
                        events that have taken place at MIT. Using a combination of different sensor
                        technologies (BLE beacons and built-in phone sensors), we determine the exhibit
                        the user is closest to and provide the user with relevant information. 
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_0674.jpg">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <img src="images/noun_212277_cc.svg">
                        <h5>Collaborative Automated Collision Resolution for Self-Driving Cars</h5>
                        <h6>Dillon Dumesnil, Jordan Lucier, Alexander Nordin</h6>
                    </td>
                    <td class="buttons">
                        <a target="_blank" href="posters/dumesnil-lucier-nordin.pdf">Slides<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://youtu.be/1jEQFhUjNEI">Video<br>Demo<img class="video" src="images/video.svg"></a>
                        <a target="_blank" href="https://github.com/jlucier/6.S062-car-swarm">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                        <a target="_blank" href="https://github.com/jlucier/car_tracker">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td>
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        In the not too distant future, self-driving cars will be a ubiquitous feature
                        of society.  We propose a project creating a system and protocol under which
                        autonomous vehicles, such as self-driving cars, can communicate and work
                        together to navigate their environment.  Our project tackles the issue of
                        resolving collisions between cars in a fair and efficient manner.  We
                        incorporated functionality for our cars to interact properly with pedestrians
                        and high-priority vehicles, such as firetrucks and police cars. Our results
                        will include a showcase featuring video of our cars safely resolving collisions
                        with other cars and pedestrians as well as our RC cars and their modified
                        hardware kits.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_0672.jpg">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <img src="images/noun_15818_cc.svg">
                        <h5>SureLock: Intention-based BLE Locking Scheme</h5>
                        <h6>Austin Freel, Andrew Titus</h6>
                    </td>
                    <td class="buttons">
                        <a target="_blank" href="posters/freel-titus.pdf">Poster<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://youtu.be/MFM_eqwavqg">Video<br>Demo<img class="video" src="images/video.svg"></a>
                        <a target="_blank" href="https://github.com/atitus5/6s062-Project">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                        <a target="_blank" href="https://github.com/atitus5/6s062-Project/tree/master/logs">Data<br>Repo<img class="data" src="images/data.svg"></a>
                    </td>
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        The vast majority of current door lock systems involve some sort of physical
                        interaction with the lock. A hands-free system would require not only a
                        sufficiently secure wireless credential transfer, but also accurate detection
                        of a userâ€™s intention to attempt an unlock of the door. We developed such a
                        system using Bluetooth Smart 4.2 and a novel model using acceleration magnitude
                        and RSSI data to detect a userâ€™s intentions to unlock a door. We also propose a
                        learning model to dynamically adjust the thresholds of this model to adapt to
                        the specific environment of a given lock.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_0665.jpg">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <img src="images/noun_110488_cc.svg">
                        <h5>Air gesture recognizer on mobile devices</h5>
                        <h6>Hayley Song, Chongyuan Xiang</h6>
                    </td>
                    <td class="buttons">
                        <a target="_blank" href="posters/song-xiang.pdf">Slides<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://www.youtube.com/watch?v=jmgfW-EGMBM">Video<br>Demo<img class="video" src="images/video.svg"></a>
                        <a target="_blank" href="https://github.com/xiangcy/AirGestureClassifier">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td>
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        Currently, most users interact with their mobile devices on the screen via, for
                        example, touch and scrolling. Consequently, many applications build accurate 2D
                        handwritten digit recognizers. For our project, we aim to extend the scope to
                        three dimension. We built an iOS application that recognizes 3D gestures while
                        holding the mobile device. We use accelerometer data and make the prediction on
                        a time sequence by applying k-Nearest-Neighbor algorithm on the training data
                        with Dynamic Time Warping distance as the distance metric. We collected air
                        gestures of 10 distinct alphabet letters (O ,I, J, L, Z, S, V, T, X, B) from 10
                        users, and select exemplary points from them for training purpose. Our accuracy
                        results is on average 90.5% and the latency is 1-2 seconds per gesture.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_0671.jpg">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <img src="images/noun_28242_cc.svg">
                        <h5>AirMuler: An Anonymous Data Muling Framework</h5>
                        <h6>Ankush Gupta, Justin Martinez</h6>
                    </td>
                    <td class="buttons">
                        <a target="_blank" href="posters/gupta-martinez.pdf">Slides<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://github.com/ankushg/airmuler">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td>
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        Currently, frameworks for developing mobile applications that take advantage of
                        multi-hop networks over Bluetooth Low Energy (BLE) are limited to networks that
                        have a direct path from the source to the destination at the time of sending.
                        Additionally, in traditional data muling applications, messages and
                        sender/recipient information are regularly transmitted in plaintext. We have
                        created an iOS framework that allows developers to create applications that can
                        transport message across dynamic multihop networks which change over time,
                        while eliminating the exposure of message contents and metadata to
                        participating nodes in the multihop network. Our framework encrypts messages
                        such that neither message contents nor sender/recipient identities are leaked
                        as a result of transmission. Additionally, through the use of a unique ACKing
                        protocol implementing a zero-knowledge proof, we are able to minimize
                        retransmission once a message has been delivered, while still protecting the
                        message contents and metadata. In conjunction with our framework, we have
                        developed a simple multi-peer chat application illustrating a use case of
                        AirMuler.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_0666.jpg">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <img src="images/noun_2108_cc.svg">
                        <h5>Matching Inertial Logs to Video Tracking for Indoor Positioning</h5>
                        <h6>Eric Lau, Geronimo Mirano, Harihar Subramanyam</h6>
                    </td>
                    <td class="buttons">
                        <a target="_blank" href="posters/lau-mirano-subramanyam.pdf">Poster<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://www.youtube.com/playlist?list=PLFIqHUP4pFcmnJPeBiSrUFxtCPGzVGleV">Video<br>Demo<img class="video" src="images/video.svg"></a>
                        <a target="_blank" href="https://github.com/hariharsubramanyam/ObjectTracker">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                        <a target="_blank" href="https://github.com/hariharsubramanyam/TrackerData">Data<br>Repo<img class="data" src="images/data.svg"></a>
                    </td>
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        Suppose you own a shopping mall and you wish to provide users with a &ldquo;Map&rdquo; app,
                        complete with &ldquo;You Are Here&rdquo; marker and informative advertisements of nearby
                        attractions. Such a system requires precise indoor positioning, to a resolution
                        which modern techniques such as GPS or WiFi localization cannot achieve. We
                        present a novel system for indoor positioning which fuses overhead camera data
                        with smartphone's inertial sensor to localize a user. The system tracks the
                        trajectories of all people visible within the camera frame, then seeks to match
                        the userâ€™s inertial profile against the trajectories. The system is designed to
                        be generalizable to any client with a smartphone; no pre-registration is
                        required and user privacy is respected, both of which are improvements over a
                        system which relies on visual recognition of users. We demonstrate a high
                        fidelity vision processing system capable of tracking moving objects for many
                        frames, then show our preliminary results matching inertial sense data against
                        vision trajectories with high fidelity.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_0676.jpg">
                    </td>
                </tr>

                <tr>
                    <td class="title">
                        <img src="images/noun_143916_cc.svg">
                        <h5>Lookout: A Distributed Sensing System for Arduino and Raspberry Pi</h5>
                        <h6>Katie Siegel, Rachel Wang</h6>
                    </td>
                    <td class="buttons">
                        <a target="_blank" href="posters/siegel-wang.pdf">Poster<br>(PDF)<img class="pdf" src="images/pdf.svg"></a>
                        <a target="_blank" href="https://github.com/rswang/6s062-mob">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                        <a target="_blank" href="https://github.com/kathrynsiegel/6MOB-project-arduino">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                        <a target="_blank" href="https://github.com/kathrynsiegel/6MOB-project-pi">Code<br>Repo<img class="repo" src="images/repo.svg"></a>
                    </td>
                </tr>
                <tr>
                    <td class="left">
                        <p>
                        Lookout is a distributed sensing system that provides a cost-effective, easily
                        deployable, and extensible platform to build applications using sensor data.
                        The system has three components: sensing motes, a central gateway, and a remote
                        server. Each mote collects data from various sensors and transmits readings
                        through RF transceivers to a central Raspberry Pi gateway. The gateway forwards
                        these readings to a cloud server via API POST requests. To make the system more
                        efficient, we implemented and analyzed several optimizations for aggregating
                        and filtering data on the server and the motes. Overall, the systemâ€™s general
                        interface is easily compatible with other applications and different sensor
                        types. We have developed an example application to monitor meeting rooms, which
                        can provide information about their availability, the consistency between usage
                        and the reservation system, and temperature regulation.
                        </p>
                    </td>
                    <td class="right">
                        <img src="images/photos/IMG_0678.jpg">
                    </td>
                </tr>
            </table>

        <section id="main_content" class="inner">
            <p style="color:#808080; font-size:80%; line-height: 1.2em;">
            Images from the Noun project:<br>
            Barbell by Dara Ullrich<br>
            Shake Phone by Web Icon Set<br>
            Massachusetts Institute of Technology by NATAPON CHANTABUTR<br>
            Self-driving car by Jakob Vogel<br>
            entrance by Marc Andre Roy<br>
            Bump to Right by Web Icon Set<br>
            Donkey by Luis Prado<br>
            Tracking Location by Friedrich Santana<br>
            internet of things by Felix Westphal
            PDF File by Laurent Canivet
            Video by Milky - Digital innovation
            Graph by Simple Icons
            </p>

        </section>
    </div>
</body>
</html>
